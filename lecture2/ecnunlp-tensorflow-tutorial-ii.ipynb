{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Goal\n",
    "\n",
    "- Component in Tensorflow\n",
    "  - RNN: LSTM / GRU\n",
    "  - Attention Mechanism\n",
    "  - GAN\n",
    "\n",
    "- Next: \n",
    "  - Pointer-Generator Networks\n",
    "  - Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "- data\n",
    "    - [784] -> [10]\n",
    "\n",
    "- model [LR/SVM/etc]\n",
    "  - train\n",
    "  - predict\n",
    "\n",
    "- evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap (Cont.)\n",
    "\n",
    "- Model\n",
    "\n",
    "```python\n",
    "# define the placeholder\n",
    "\n",
    "# define the variables\n",
    "\n",
    "# build the model graph\n",
    "# including: predict, loss, and train_op\n",
    "```\n",
    "\n",
    "- Homework: Text Classification Task\n",
    "  - [model](https://github.com/rgtjf/tf-classification/blob/master/src/models/NBoW.py)\n",
    "  - [data](https://github.com/rgtjf/tf-classification/blob/master/src/data.py)\n",
    "  - [main](https://github.com/rgtjf/tf-classification/blob/master/src/main.py)\n",
    "\n",
    "- EASY to change to LSTM, HOW?\n",
    "  - [model](https://github.com/rgtjf/tf-classification/blob/master/src/models/LSTM.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def BiLSTM(input_x, input_x_len, hidden_size, num_layers=1, dropout_rate=None, return_sequence=True):\n",
    "    \"\"\"\n",
    "    BiLSTM Layer\n",
    "    Args:\n",
    "      input_x: [batch, sent_len, emb_size]\n",
    "      input_x_len: [batch, ]\n",
    "      hidden_size: int\n",
    "      num_layers: int\n",
    "      dropout_rate: float\n",
    "      return_sequence: True/False\n",
    "    Returns:\n",
    "      if return_sequence=True:\n",
    "          outputs: [batch, sent_len, hidden_size*2]\n",
    "      else:\n",
    "          output: [batch, hidden_size*2]\n",
    "    \"\"\"\n",
    "    # cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    cell_fw = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "    cell_bw = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        # Warning! Please consider that whether the cell to stack are the same\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell([cell_fw for _ in range(num_layers)])\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell([cell_bw for _ in range(num_layers)])\n",
    "\n",
    "    if dropout_rate:\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=(1 - dropout_rate))\n",
    "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=(1 - dropout_rate))\n",
    "\n",
    "    b_outputs, b_states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_x,\n",
    "                                                            sequence_length=input_x_len, dtype=tf.float32)\n",
    "    if return_sequence:\n",
    "        outputs = tf.concat(b_outputs, axis=2)\n",
    "    else:\n",
    "        # states: [c, h]\n",
    "        outputs = tf.concat([b_states[0][1], b_outputs[1][1]], axis=-1)\n",
    "    return outputs\n",
    "\n",
    "with tf.variable_scope(\"bilstm\") as s:\n",
    "    lstm_x = BiLSTM(embedded_x, self.input_x_len, self.lstm_size, return_sequence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNCell\n",
    "$h_t = \\sigma(W^{(hh)}h_{t-1}+W^{(hx)}x_t) + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicRNNCell(RNNCell):\n",
    "    \"\"\"The most basic RNN cell.\"\"\"\n",
    "    def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "        with vs.variable_scope(scope or \"basic_rnn_cell\"):\n",
    "            output = self._activation(\n",
    "              _linear([inputs, state], self._num_units, True, scope=scope))\n",
    "        return output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRUCell\n",
    "$r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{t-1} + b^{(r)})$\n",
    "\n",
    "$u_t = \\sigma(W^{(u)}x_t + U^{(u)}h_{t-1} + b^{(u)})$\n",
    "\n",
    "$c = \\tanh\\big( W^{(c)}x_t   + U^{(c)}(r_t \\odot h_{t-1})  + b^{(c)}\\big)$\n",
    "\n",
    "$h_t = u_t \\odot h_{t-1} + (1-u_t) \\odot c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUCell(RNNCell):\n",
    "    \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n",
    "    def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "        self._num_units = num_units\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "        with vs.variable_scope(scope or \"gru_cell\"):\n",
    "            with vs.variable_scope(\"gates\"):  # Reset gate and update gate.\n",
    "            # We start with bias of 1.0 to not reset and not update.\n",
    "            r, u = array_ops.split(\n",
    "                value=_linear(\n",
    "                    [inputs, state], 2 * self._num_units, True, 1.0, scope=scope),\n",
    "                num_or_size_splits=2,\n",
    "                axis=1)\n",
    "            r, u = sigmoid(r), sigmoid(u)\n",
    "            with vs.variable_scope(\"candidate\"):\n",
    "            c = self._activation(_linear([inputs, r * state], self._num_units, True, scope=scope))\n",
    "            new_h = u * state + (1 - u) * c\n",
    "        return new_h, new_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMCell\n",
    "\n",
    "$i_t = \\sigma(W^{(i)}x_t + U^{(i)}h_{t-1} + b^{(i)})$\n",
    "\n",
    "$f_t = \\sigma(W^{(f)}x_t + U^{(f)}h_{t-1} + b^{(f)})$\n",
    "\n",
    "$o_t = \\sigma(W^{(o)}x_t + U^{(o)}h_{t-1} + b^{(o)})$\n",
    "\n",
    "$\\tilde{c_t} = \\tanh(W^{(c)}x_t + U^{(c)}h_{t-1} + b^{(c)}))$\n",
    "\n",
    "$c_t = i_t \\odot \\tilde{c_t} + f_t \\odot c_{t-1}$\n",
    "\n",
    "$h_t = o_t \\odot \\tanh(c_t)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicLSTMCell(RNNCell):\n",
    "    \"\"\"Basic LSTM recurrent network cell.\n",
    "    The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
    "    We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
    "    reduce the scale of forgetting in the beginning of the training.\n",
    "    It does not allow cell clipping, a projection layer, and does not\n",
    "    use peep-hole connections: it is the basic baseline.\n",
    "    For advanced models, please use the full LSTMCell that follows.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_units, forget_bias=1.0, input_size=None,\n",
    "               state_is_tuple=True, activation=tanh):\n",
    "    \"\"\"Initialize the basic LSTM cell.\n",
    "    Args:\n",
    "      num_units: int, The number of units in the LSTM cell.\n",
    "      forget_bias: float, The bias added to forget gates (see above).\n",
    "      input_size: Deprecated and unused.\n",
    "      state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "        the `c_state` and `m_state`.  If False, they are concatenated\n",
    "        along the column axis.  The latter behavior will soon be deprecated.\n",
    "      activation: Activation function of the inner states.\n",
    "    \"\"\"\n",
    "        if not state_is_tuple:\n",
    "            logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "                       \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "        self._num_units = num_units\n",
    "        self._forget_bias = forget_bias\n",
    "        self._state_is_tuple = state_is_tuple\n",
    "        self._activation = activation\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return (LSTMStateTuple(self._num_units, self._num_units) if self._state_is_tuple else 2 * self._num_units)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
    "        with vs.variable_scope(scope or \"basic_lstm_cell\"):\n",
    "            # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "            if self._state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n",
    "            concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\n",
    "\n",
    "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "            i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)\n",
    "\n",
    "            new_c = (c * sigmoid(f + self._forget_bias) + sigmoid(i) * self._activation(j))\n",
    "            new_h = self._activation(new_c) * sigmoid(o)\n",
    "\n",
    "            if self._state_is_tuple:\n",
    "                new_state = LSTMStateTuple(new_c, new_h)\n",
    "            else:\n",
    "                new_state = array_ops.concat([new_c, new_h], 1)\n",
    "            return new_h, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "$\\alpha_i = softmax_i q^T W p_i$\n",
    "\n",
    "$o  = \\sum_i \\alpha_i p_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bilinear_attention(question_rep, passage_repres, passage_mask):\n",
    "    \"\"\"\n",
    "    Attention bilinear\n",
    "    ref: https://arxiv.org/pdf/1606.02858v2.pdf\n",
    "         https://github.com/danqi/rc-cnn-dailymail/blob/master/code/nn_layers.py\n",
    "    $\\alpha_i = softmax_i q^T W p_i$\n",
    "    $ o  = \\sum_i \\alpha_i p_i $\n",
    "\n",
    "      Args:\n",
    "        question_rep: [batch_size, hidden_size]\n",
    "        passage_repres: [batch_size, sequence_length, hidden_size]\n",
    "        passage_mask: [batch_size, sequence_length]\n",
    "      Returns:\n",
    "        passage_rep: [batch_size, hidden_size]\n",
    "    \"\"\"\n",
    "    hidden_size = question_rep.get_shape()[1]\n",
    "    # [hidden_size, hidden_size]\n",
    "    W_bilinear = tf.get_variable(\"W_bilinear\", shape=[hidden_size, hidden_size], dtype=tf.float32)\n",
    "\n",
    "    # [batch_size, hidden_size]\n",
    "    question_rep = tf.matmul(question_rep, W_bilinear)\n",
    "    \n",
    "    # [batch_size, 1, hidden_size]\n",
    "    question_rep = tf.expand_dims(question_rep, 1)\n",
    "    \n",
    "    # [batch_size, seq_length]\n",
    "    alpha = tf.nn.softmax(tf.reduce_sum(question_rep * passage_repres, axis=2))\n",
    "    alpha = alpha * passage_mask\n",
    "    alpha = alpha / tf.reduce_sum(alpha, axis=-1, keep_dims=True)\n",
    "\n",
    "    # [batch_size, hidden_size]\n",
    "    passage_rep = tf.reduce_sum(passage_repres * tf.expand_dims(alpha, axis=-1), axis=1)\n",
    "    return passage_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
