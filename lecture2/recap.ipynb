{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow-tutorial-i-recap\n",
    "\n",
    "### Big idea: express a numeric computation as a graph.\n",
    "\n",
    "- graph nodes are **operations** which have any number of inputs and outputs\n",
    "- Graph edges are **tensors** which flow between nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![graph](https://github.com/rgtjf/ecnunlp-tensorflow-tutorial/raw/master/lecture2/figs/graph.jpg)\n",
    "\n",
    "# $$h = ReLU(Wx + b)$$ \n",
    "\n",
    "- placeholder\n",
    "- Variable\n",
    "- Operation\n",
    "\n",
    "```python\n",
    "        # tf Graph Input\n",
    "        x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "        y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "        # Set model weights\n",
    "        W1 = tf.Variable(tf.zeros([784, 100]), name='W1')\n",
    "        b1 = tf.Variable(tf.zeros([100]), name='b1')\n",
    "\n",
    "        h = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "        \n",
    "        W2 = tf.Variable(tf.zeros([100, 10]), name='W2')\n",
    "        b2 = tf.Variable(tf.zeros([10]), name='b2')\n",
    "        \n",
    "        # Construct model\n",
    "        pred = tf.nn.softmax(tf.matmul(h, W2) + b2) # Softmax\n",
    "\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "        # Gradient Descent\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "        # session\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(1000):\n",
    "            batch_x, batch_label = data.next_batch()\n",
    "            sess.run(train_step, feed_dict={x: batch_x, y: batch_label}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast\n",
    "\n",
    "```\n",
    "A      (3d array):  15 x 3 x 5\n",
    "B      (2d array):       3 x 1\n",
    "```\n",
    "\n",
    "## Conv\n",
    "\n",
    "```python\n",
    "conv2d(\n",
    "    input,\n",
    "    filter,\n",
    "    strides,\n",
    "    padding,\n",
    "    use_cudnn_on_gpu=None,\n",
    "    data_format=None,\n",
    "    name=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "class Data(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        mnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)\n",
    "        \n",
    "        self.mnist = mnist\n",
    "        \n",
    "    def batch_iter(self, batch_size):\n",
    "        return  self.mnist.train.next_batch(batch_size)\n",
    "    \n",
    "    def test_data(self):\n",
    "        x = self.mnist.test.images[:3000]\n",
    "        y = self.mnist.test.labels[:3000]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.mnist.train.num_examples\n",
    "\n",
    "    \n",
    "        \n",
    "class Model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # tf Graph Input\n",
    "        x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "        y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "        # Set model weights\n",
    "        W1 = tf.Variable(tf.zeros([784, 100]), name='W1')\n",
    "        b1 = tf.Variable(tf.zeros([100]), name='b1')\n",
    "\n",
    "        h = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "        \n",
    "        W2 = tf.Variable(tf.zeros([100, 10]), name='W2')\n",
    "        b2 = tf.Variable(tf.zeros([10]), name='b2')\n",
    "        \n",
    "        # Construct model\n",
    "        pred = tf.nn.softmax(tf.matmul(h, W2) + b2) # Softmax\n",
    "\n",
    "        # Minimize error using cross entropy\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "        # Gradient Descent\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.pred = pred\n",
    "        self.cost = cost\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    \n",
    "    def train_model(self, sess, batch_xs, batch_ys):\n",
    "        feed_dict = {\n",
    "            self.x: batch_xs,\n",
    "            self.y: batch_ys\n",
    "        }\n",
    "        to_return = {\n",
    "            'train_op': self.optimizer,\n",
    "            'loss': self.cost\n",
    "        }\n",
    "        return sess.run(to_return, feed_dict)\n",
    "\n",
    "    def test_model(self, sess, batch_xs):\n",
    "        feed_dict = {\n",
    "            self.x: batch_xs\n",
    "        }\n",
    "        to_return = {\n",
    "            'pred': self.pred\n",
    "        }\n",
    "        return sess.run(to_return, feed_dict)\n",
    "        \n",
    "\n",
    "    def eval_model(self, sess, batch_xs, batch_ys):\n",
    "         # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.y, 1))\n",
    "        # Calculate accuracy for 3000 examples\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        feed_dict = {\n",
    "            self.x: batch_xs,\n",
    "            self.y: batch_ys\n",
    "        }\n",
    "        to_return = {\n",
    "            'acc': accuracy\n",
    "        }\n",
    "        return sess.run(to_return, feed_dict)\n",
    "        \n",
    "data = Data()\n",
    "model = Model()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(data) / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = data.batch_iter(batch_size)\n",
    "            # Fit training using batch data\n",
    "            results = model.train_model(sess, batch_xs, batch_ys)\n",
    "            c = results['loss']\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    test_xs, test_ys = data.test_data()\n",
    "    acc = model.eval_model(sess, test_xs, test_ys)\n",
    "    print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "- data\n",
    "    - [784] -> [10]\n",
    "\n",
    "- model [LR/SVM/etc]\n",
    "  - train\n",
    "  - predict\n",
    "\n",
    "- evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap (Cont.)\n",
    "\n",
    "- Model\n",
    "\n",
    "```python\n",
    "# define the placeholder\n",
    "\n",
    "# define the variables\n",
    "\n",
    "# build the model graph\n",
    "# including: predict, loss, and train_op\n",
    "```\n",
    "\n",
    "- Text Classification Task\n",
    "  - [model](https://github.com/rgtjf/tf-classification/blob/master/src/models/NBoW.py)\n",
    "  - [data](https://github.com/rgtjf/tf-classification/blob/master/src/data.py)\n",
    "  - [main](https://github.com/rgtjf/tf-classification/blob/master/src/main.py)\n",
    "\n",
    "- EASY to change to LSTM, HOW?\n",
    "  - [model](https://github.com/rgtjf/tf-classification/blob/master/src/models/LSTM.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
